---
title: Dérivation de l'estimateur OLS
date: '2020-12-21'
language: fr
localeid: 'derivingols'
categories: ['code', 'math']
tags: ['next-js', 'ols']
image: '/images/image-placeholder.png'
draft: false
summary: Comment dériver l'estimateur OLS avec la notation matricielle et une visite guidée de la composition mathématique, en utilisant markdown et à l'aide de KaTeX.
---

# Introduction

L'analyse et l'affichage des équations mathématiques sont inclus dans ce modèle de blog. L'analyse mathématique est activée par `remark-math` et `rehype-katex`.
KaTeX et sa police associée sont inclus dans `_document.js` alors n'hésitez pas à l'utiliser sur n'importe quelle page.
[^footnote]

[^footnote]: Pour la liste complète des fonctions TeX prises en charge, consultez la [documentation KaTeX](https://katex.org/docs/supported.html)

Les symboles mathématiques en ligne peuvent être inclus en plaçant le terme entre le symbole «$».

Les blocs de code mathématique sont désignés par `$$`.

Si vous avez l'intention d'utiliser le signe `$` au lieu des mathématiques, vous pouvez l'échapper (`\$`) ou spécifier l'entité HTML (`&dollar;`) [^2]

Les notes de bas de page en ligne ou énumérées manuellement sont également prises en charge. Cliquez sur les liens ci-dessus pour les voir en action.

[^2]: \$10 et &dollar;20.

# Dérivation de l'estimateur OLS

En utilisant la notation matricielle, soit $n$ le nombre d'observations et $k$ le nombre de régresseurs.

Le vecteur des variables de résultat $\mathbf{Y}$ est une matrice $n \times 1$,

```tex
\mathbf{Y} = \left[\begin{array}
  {c}
  y_1 \\
  . \\
  . \\
  . \\
  y_n
\end{array}\right]
```

$$
\mathbf{Y} = \left[\begin{array}
  {c}
  y_1 \\
  . \\
  . \\
  . \\
  y_n
\end{array}\right]
$$

La matrice des régresseurs $\mathbf{X}$ est une matrice $n \times k$ (ou chaque ligne est un vecteur $k \times 1$),

```latex
\mathbf{X} = \left[\begin{array}
  {ccccc}
  x_{11} & . & . & . & x_{1k} \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  x_{n1} & . & . & . & x_{nn}
\end{array}\right] =
\left[\begin{array}
  {c}
  \mathbf{x}'_1 \\
  . \\
  . \\
  . \\
  \mathbf{x}'_n
\end{array}\right]
```

$$
\mathbf{X} = \left[\begin{array}
  {ccccc}
  x_{11} & . & . & . & x_{1k} \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  . & . & . & . & .  \\
  x_{n1} & . & . & . & x_{nn}
\end{array}\right] =
\left[\begin{array}
  {c}
  \mathbf{x}'_1 \\
  . \\
  . \\
  . \\
  \mathbf{x}'_n
\end{array}\right]
$$

Le vecteur des termes d'erreur $\mathbf{U}$ est également une matrice $n \times 1$.

Parfois, il peut être plus facile d'utiliser la notation vectorielle. Par souci de cohérence, j'utiliserai le petit x gras pour désigner un vecteur et les lettres majuscules pour désigner une matrice. Les observations uniques sont indiquées par l'indice.

## Moindres carrés

**Start**:  
$$y_i = \mathbf{x}'_i \beta + u_i$$

**Assumptions**:

1. Linearity (given above)
2. $E(\mathbf{U}|\mathbf{X}) = 0$ (conditional independence)
3. rank($\mathbf{X}$) = $k$ (no multi-collinearity i.e. full rank)
4. $Var(\mathbf{U}|\mathbf{X}) = \sigma^2 I_n$ (Homoskedascity)

**Aim**:  
Find $\beta$ that minimises the sum of squared errors:

$$
Q = \sum_{i=1}^{n}{u_i^2} = \sum_{i=1}^{n}{(y_i - \mathbf{x}'_i\beta)^2} = (Y-X\beta)'(Y-X\beta)
$$

**Solution**:  
Hints: $Q$ is a $1 \times 1$ scalar, by symmetry $\frac{\partial b'Ab}{\partial b} = 2Ab$.

Take matrix derivative w.r.t $\beta$:

```tex
\begin{aligned}
  \min Q           & = \min_{\beta} \mathbf{Y}'\mathbf{Y} - 2\beta'\mathbf{X}'\mathbf{Y} +
  \beta'\mathbf{X}'\mathbf{X}\beta \\
                   & = \min_{\beta} - 2\beta'\mathbf{X}'\mathbf{Y} + \beta'\mathbf{X}'\mathbf{X}\beta \\
  \text{[FOC]}~~~0 & =  - 2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\hat{\beta}                  \\
  \hat{\beta}      & = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}                              \\
                   & = (\sum^{n} \mathbf{x}_i \mathbf{x}'_i)^{-1} \sum^{n} \mathbf{x}_i y_i
\end{aligned}
```

$$
\begin{aligned}
  \min Q           & = \min_{\beta} \mathbf{Y}'\mathbf{Y} - 2\beta'\mathbf{X}'\mathbf{Y} +
  \beta'\mathbf{X}'\mathbf{X}\beta \\
                   & = \min_{\beta} - 2\beta'\mathbf{X}'\mathbf{Y} + \beta'\mathbf{X}'\mathbf{X}\beta \\
  \text{[FOC]}~~~0 & =  - 2\mathbf{X}'\mathbf{Y} + 2\mathbf{X}'\mathbf{X}\hat{\beta}                  \\
  \hat{\beta}      & = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}                              \\
                   & = (\sum^{n} \mathbf{x}_i \mathbf{x}'_i)^{-1} \sum^{n} \mathbf{x}_i y_i
\end{aligned}
$$
